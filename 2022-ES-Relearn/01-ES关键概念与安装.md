# ES关键概念

## 1-使用MySQL实现搜索场景

## 先回顾一下MySQL的聚簇索引和辅助索引，就以以下表信息为例：

|  id  |   title    | price |
| :--: | :--------: | :---: |
|  1   |  小米手机  | 3499  |
|  2   |  华为手机  | 5499  |
|  3   | 华为充电器 |  48   |
|  4   |  小米手环  |  299  |

假设id是PRIMARY KEY，字段titile有索引，也就是说这张表有id的聚簇索引和titile的辅助索引，聚簇索引叶子节点存的是改行记录的信息，辅助索引叶子节点存的是PRIMARY KEY的值。如果在搜索商品标题的业务场景下，sql语句是这样写的：

```mysql
SELECT * FROM table WHERE titile LIKE '%手机%';
```

这种全模糊的搜索方式是不会命中title索引的，除非是 ='手机'或LIKE '手机%'，但显然这种搜索方式明显不符合业务需求。如果要用MySQL来实现这种场景的话，就只能走全表扫描，这是不应该出现的。

## 2-倒排索引

基于知识点1，就要提到ES的倒排索引了，那倒排索引是如何实现的呢？这就先要了解ES的两个概念：文档、词条

文档好理解，之前学ES就知道了文档的概念，那词条呢？

词条（term）可以理解为文档里的数据经过**分词器处理**后过滤出来的数据，比如小米手机，经过分词器处理后可以形成“小米”、“手机”这两个词。

其实倒排索引有点类似辅助索引，不过倒排索引是以**字段值分词后的词**来作检索条件，词对应的值也是文档id，不过可以有多个。按照知识点1的例子来看，最终这四个文档会维护以下倒排索引：

|  词条  | 文档id |
| :----: | :----: |
|  小米  |  1,4   |
|  华为  |  2,3   |
|  手环  |   4    |
| 充电器 |   3    |
|  手机  |  1,2   |

这样，当搜索手机时，只需通过倒排索引将手机对应的文档id=1和2找到，再通过id=1和2找到对应的文档内容，就能既满足商品标题的业务场景，又不会走全表扫描这种丢失性能的做法。

**当然，倒排索引是基于分词器来划分词条的，如果字段对应的属性是keyword，就无法被分词，直接将实际值作为词条。**

## 2-与MySQL的区别

除了知识点1和2的区别外，ES的一些概念和MySQL也是有点相似但不同的，**比如ES的数据最终是基于JSON存储的，**比如：

| MYSQL  |    ES    |                      DESC                      |
| :----: | :------: | :--------------------------------------------: |
| Table  |  Index   | 索引，这里的索引不是倒排索引，而是类似表的概念 |
|  Row   | Document |                      文档                      |
| Column |  Field   |                   JSON的key                    |
| Schema | Mapping  |                 对Field的约束                  |
|  SQL   |   DSL    |  本质是JSON风格的Restful请求语句，用来操作ES   |

值得一提的是，ES和MySQL并不是取代关系，MySQL更加注重数据的硬盘保存和**事务操作**，如一个授信流程，里面涉及到一个事务对多张表的操作，这一点是ES无法实现的。ES更加注重于从海量数据中通过关键字搜索、分析。**实际上，它们属于互补关系**。

# ES的安装

## 3-使用docker安装ES

1. 创建ES网络，保证单点互联

   docker network create es-net

2. 基于本地镜像加载ES镜像

   因为ES镜像非常大，所以最好提前下载好再加载进去：docker load -i es.tar

3. 创建ES容器

   ```bash
   docker run -d \
   	--name es \
   	-e 'ES_JAVA_OPTS=-Xms512M -Xmx512M' \ 设置ES的Java堆内存
   	-e 'discovery.type=single-node' \	设置ES是单点运行，非集群
   	-v es-data:/usr/share/elasticsearch/data \	设置数据卷挂载，自动生成
   	-v es-plugins:/usr/share/elasticsearch/plugins \	设置数据卷挂载，自动生成
   	--privileged \ 使容器内的root权限拥有宿主机的root权限
   	--network es-net \ 明确规定容器使用es-net网络
   	-p 9200:9200 \ 暴露ES对外的Restful操作端口
   	-p 9300:9300 \ 暴露ES之间互联的端口，不过单机运行，这个端口不用不上
   	elasticsearch:7.12.1
   ```

4. 安装成功后，curl一下9200端口，可以看到以下信息：

```bash
kjg@kjg-PC:~$ curl http://localhost:9200
{
  "name" : "8339abb9f0bc",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "7UfyhiOmSRuIXyeE9789mA",
  "version" : {
    "number" : "7.12.1",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "3186837139b9c6b6d23c3200870651f10d3343b7",
    "build_date" : "2021-04-20T20:56:39.040728659Z",
    "build_snapshot" : false,
    "lucene_version" : "8.8.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
```

# Kibana的安装

## 4-使用docker安装Kibana

1. 导入kibana镜像

   docker load -i kibana.tar

2. 创建kibana容器

   ```bash
   docker run -d \
   	--name kibana \
   	-e ELASTICSEARCH_HOSTS=http://es:9200 \指定es的地址，这里的es指的是容器名
   	--network es-net \
   	-p 5601:5601 \
   	kibana:7.12.1 注意 kibana的版本要和es版本一致
   ```

# 5-ik分词器的安装

ES默认的分词器对中文支持不太友好，以下用KIBANA试一试：

![image](https://user-images.githubusercontent.com/48977889/167280502-6e2f03bb-9308-44af-b5c7-468889fdbb44.png)

可以看到一段中文短语直接被单字拆分了，这显然是不行的

直接将ik分词器的插件安装到插件目录里，然后重启es：

```bash
root@kjg-PC:/home/kjg/enviroments# cp -r ik/ /var/lib/docker/volumes/es-plugins/_data
root@kjg-PC:/home/kjg/enviroments# docker restart es
es
root@kjg-PC:/home/kjg/enviroments#
```

此时再用ik_max_word或ik_smart试试分词：

![截图_选择区域_20220508113555](https://user-images.githubusercontent.com/48977889/167280671-558ad7b2-10a9-45de-a336-c942d6346b95.png)

![截图_选择区域_20220508113616](https://user-images.githubusercontent.com/48977889/167280676-6aabbf89-9fb7-4d16-832d-8be9bb1c90ed.png)

可以看到分词效果展现出来了，只不过这个短语比较难分，也就只能分出你好、好呀这两个词。